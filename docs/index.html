<!DOCTYPE html>
<html class="sl-theme-dark" id="base">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Thesis</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@shoelace-style/shoelace@2.0.0-beta.83/dist/themes/light.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@shoelace-style/shoelace@2.0.0-beta.83/dist/themes/dark.css" />

  <link type="text/css" rel="stylesheet" href="main.css" />
  <script type="module" src="https://cdn.jsdelivr.net/npm/@shoelace-style/shoelace@2.0.0-beta.83/dist/shoelace.js"></script>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;1,300;1,400&display=swap" rel="stylesheet">
<script type="text/javascript">
  if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
    document.getElementById("base").classList.add('sl-theme-dark');
  } else {
    document.getElementById("base").classList.remove('sl-theme-dark');
  }
</script>

</head>
<body>
  <sl-switch id="theme-toggle">Toggle Dark Mode</sl-switch>

  <sl-drawer label="Overview" class="drawer-overview">
    <sl-switch id="theme-toggle-drawer">Toggle Dark Mode</sl-switch>
    <ul class="drawer-options">
      <li><a class="progress-log">Progress Log</a></li>
      <li><a class="models">Models</a></li>
    </ul>
  </sl-drawer>

  <sl-icon name="list" class="drawer-button"></sl-icon>

  <h1>Re-identification in court decisions</h1>

  <sl-tab-group placement="end" class="tabs">
    <sl-tab slot="nav" panel="log" class="nav-entry nav-progress-log">Progress Log</sl-tab>
    <sl-tab slot="nav" panel="models" class="nav-entry nav-models">Models</sl-tab>

    <sl-tab-panel name="log">
      <h2 style="font-weight: 300">Progress Log</h2>
      <div class="log-section">

        <!-- log entry -->
        <div class="log-entry">
          <h3>Analysis on preprocessing</h3>
          <div class="tags">
            <sl-badge variant="neutral">01.12.22</sl-badge>
            <sl-badge variant="primary">v1</sl-badge>
            <sl-badge variant="warning">ANALYSIS</sl-badge>
          </div>

          <ul>
            <li>Analyzed number of wiki pages remaining after preprocessing</li>
          </ul>

          <sl-details summary="Details">
            <h4>Metrics on Preprocessing</h4>
            <p>To get a reliable dataset, some wikipedia pages had to be sorted out. The maximum amount of people which could be queried from wikipedia at once (due to query / network timeouts) were 600'000 people. Searching them in the wikipedia dataset from huggingface, 4401 persons could not be found in the dataset and were sorted out. From the remaining wiki pages only those containing more than 6'000 characters were kept, others were too short to identify anyone, and sometimes did not even mention the person in the page more than once. After sorting, only a bit over 10% of pages are left (71'065). From those 2'336 had to be removed because the masking step could not mask any entity. This could have either been because of bad named entity detection, missing entites in the article or a missmatch when checking if the entities found by the named entity recognition were the right person. This could have been caused by very special names, characters or when the name of the person is usually not used, for example an article might use an artists real name as the title of the page, but within the page only their artist name is used. A total of 68'729 pages with at least one mask each is left after preparations for experiments.</p>
            <div class="img-container">
              <figure>
                <img src="images/flow.svg">
                <figcaption>Lost wikipedia pages during preprocessing.</figcaption>
              </figure>
            </div>
          </sl-details>
        </div>

        <!-- log entry -->
        <div class="log-entry">
          <h3>Demasking setup for model comparison</h3>
          <div class="tags">
            <sl-badge variant="neutral">29.11.22</sl-badge>
            <sl-badge variant="primary">v1.3</sl-badge>
            <sl-badge variant="warning">REFACTORING</sl-badge>
          </div>

          <ul>
            <li>Changed fill-mask task to use huggingface datasets</li>
            <li>Added options to select different models to predict masks</li>
            <li>Built splitter class to split text to predict in different ways</li>
          </ul>

          <sl-details summary="Details">
            <h4>Model options</h4>
            <p>To find the best model for predicting masks depending on the knowledge base ingested during training, different models are evaluated. First model is roberta-base, others are to come.</p>
            <h4>Splitter</h4>
            <p>A helper class is supposed to split texts into batches that can easily be processed by the model. The splits are parametrized mainly by the number of sequences per batch and the length of each sequence in the batch. The number of sequences per batch is only important for the speed at which the model can process inputs. Larger batches lead to faster processing, however they do require more memory. Depending on the size of each sequence in the batch, the number of sequences in the batch (e.g. the batch-size) has to be adjusted so the memory on the GPU is not exhausted.</br>
            The length of a sequence can be too long, meaning it overshoots the maximum number of tokens allowed for a model. This stands in contrast with the desire to have the largest possible input to a model, to give the model more information for its prediction.</p>
          </sl-details>
        </div>

        <!-- log entry -->
        <div class="log-entry">
          <h3>Implement Masking for large dataset</h3>
          <div class="tags">
            <sl-badge variant="neutral">24.11.22</sl-badge>
            <sl-badge variant="primary">v1.3</sl-badge>
            <sl-badge variant="warning">REFACTORING</sl-badge>
          </div>

          <ul>
            <li>Changed masking to work on large huggingface dataset</li>
            <li>Apply advanced caching</li>
            <li>Mask the full wikipedia dataset</li>
            <li>Identified ~2k news articles which identify a court decision using regex and keywords</li>
          </ul>
        </div>

        <!-- log entry -->
        <div class="log-entry">
          <h3>Full Rebuild of Paraphrasing</h3>
          <div class="tags">
            <sl-badge variant="neutral">16.11.22</sl-badge>
            <sl-badge variant="primary">v1.3</sl-badge>
            <sl-badge variant="warning">REFACTORING</sl-badge>
          </div>

          <ul>
            <li>Out of of memory on gpu issue fixed by batching sentences independent of wiki pages.</li>
            <li>Rebuilt main loop of paraphrasing to efficiently use huggingface datasets caching.</li>
            <li>Usage of Ubelix array jobs to run jobs faster while using caching more easily.</li>
          </ul>
          <sl-details summary="Details">
            <h4>Memory Issues and sentence batching</h4>
            <p>In the first attempt pages were processed in batches and their sentences all joined to a long list to be processed by the model. This caused memory issues in some cases where by chance several larger pages were in the same batch. Instead of clipping sentences, the length of the input is now not decided by the number of pages in a batch, but the sentences are split to fit the approximate maximum input length for the model.</p>
            <p>Now pages are processed in batches, but for each badge of pages their sentences are concated to a large list, which is split up in parts which fit the size of the model, maximising the efficiency of the model, while still running stable. The overhead of splitting and rejoining text does not matter in comparison to the runtime of the paraphrasing itself.</p>
            <p>Running pages in batches also allows to cache the results easily with hugginface, as mapped functions on the dataset batches are cached by default. This allows to restart jobs if they failed at some point, without doing full recalcuations.</p>
            <h4>Ubelix array jobs</h4>
            <p>As the longest time possible to run on ubelix is 24h on the gpus required for paraphrasing, instead of creating a single 60h job, the dataset is now split into 7 parts, each containing around 10k pages. Each job then processes one such shard, splitting it again in 10 parts, resulting in a cache files for 7 jobs * 10 shards, which gives 70 savepoints during processing. Each holds around 1k wiki pages. After processing the datasets are concated back together for further processing.</p>
          </sl-details>
        </div>

        <!-- log entry -->
        <div class="log-entry">
          <h3>Paraphrasing Masks vs Normal Masks</h3>
          <div class="tags">
            <sl-badge variant="neutral">26.10.22</sl-badge>
            <sl-badge variant="primary">v1.3</sl-badge>
            <sl-badge variant="warning">REFACTORING</sl-badge>
          </div>

          <ul>
            <li>Switch processing to huggingface datasets, include caching.</li>
            <li>Fix memory issues for very long sentences, further improving paraphrasing performance.</li>
          </ul>
        </div>

        <!-- log entry -->
        <div class="log-entry">
          <h3>Paraphrasing Masks vs Normal Masks</h3>
          <div class="tags">
            <sl-badge variant="neutral">20.10.22</sl-badge>
            <sl-badge variant="primary">v1.2</sl-badge>
            <sl-badge variant="warning">ANALYSIS</sl-badge>
          </div>

          <ul>
            <li>Analyzed paraphrasing performance.</li>
            <li>Analysis of paraphrasing time.</li>
          </ul>

          <sl-details summary="Details">
          <h4>Paraphrasing time</h4>
          <p>With approximately 9 days running time for the first version, the time is now down to 9 hours.
          Increasing batch sizes, ditching sentences which are way too long and removing sections which cause the
          tokenizer to generate a lot of tokens for words it does know have in its default dictionary made the process
          much faster.</p>
          <h4>Results for Paraphrasing</h4>
          <p>Comparing the number of masks with the length of input texts,
          we find that paraphrased texts are on average one quarter shorter 
        than unparaphrased texts. The number of masks to text length is very similar between paraphrased and normal text (trendline). The average number of masks is about one quarter lower for paraphrased texts, which fits the shorter overall text length of paraphrased sentences. This is good news, it means paraphrasing does not lose any entities for reidentifaction.</p>
          <div class="img-container">
            <figure>
              <img src="images/masks_vs_text_length.png">
              <figcaption>Comparison of number of masks to length of text for normal and paraphrased text
              </figcaption>
            </figure>
          </div>
          <p>Comparing masks per sentence gives a better resolution on masks compared to their importance within sentences.</p>
          <div class="img-container">
            <figure>
              <img src="images/masks_vs_sentences_count.png">
              <figcaption>Comparison of number of masks per sentence for normal and paraphrased text in model</figcaption>
            </figure>
          </div>

        </sl-details>
      </div>

      <div class="log-entry">
        <h3>Fixing the Masking</h3>
        <div class="tags">
          <sl-badge variant="neutral">12.10.22</sl-badge>
          <sl-badge variant="primary">v1</sl-badge>
          <sl-badge variant="warning">REFACTORING</sl-badge>
        </div>

        <ul>
          <li>Switched from replacing entities in the text with regex to replacing them with the index given by NER.</li>
          <li>Added batching to NER so the tokenizer can tokenize the full input, allowing NER to detect all entities, not just within the first part of the text.</li>
        </ul>

        <sl-details summary="Details">
          <h4>Masking is now correct, we mask a word for every entity detected.</h4>
          <div class="img-container">
            <figure>
              <img src="images/mask_vs_entities_v1.png">
              <figcaption>Wrong number of masks compared to entities due to poor masking.</figcaption>
            </figure>
          </div>
          <div class="img-container">
            <figure>
              <img src="images/mask_vs_entities_v2.png">
              <figcaption>Number of masks vs number of entities after correct entity detection.</figcaption>
            </figure>
          </div>
        </sl-details>
      </div>

      </div>
    </sl-tab-panel>

    <sl-tab-panel name="models">
      <sl-alert variant="warning" open>
        <sl-icon slot="icon" name="exclamation-triangle"></sl-icon>
        <strong>Oops!</strong><br />
        Nothing here yet.
      </sl-alert>
    </sl-tab-panel>
  </sl-tab-group>


<script type="text/javascript">
  /* listener for dark mode toggle */
  document.getElementById("theme-toggle").addEventListener("sl-change", function() {
    document.getElementById("base").classList.toggle("sl-theme-dark");
    document.getElementById('theme-toggle-drawer').click();
  });
  /* listener for dark mode toggle in drawer */
  document.getElementById("theme-toggle-drawer").addEventListener("sl-change", function() {
    document.getElementById('theme-toggle').click();
  });
</script>

<script>
  const drawer = document.querySelector('.drawer-overview');
  const openButton = drawer.nextElementSibling;
  openButton.addEventListener('click', () => drawer.show());
</script>

<script type="text/javascript">
  const progressButton = document.querySelector('a.progress-log');
  const modelsButton = document.querySelector('a.models');
  const tabGroup = document.querySelector('sl-tab-group.tabs');
  progressButton.addEventListener('click', () => {drawer.hide(); tabGroup.show('log')});
  modelsButton.addEventListener('click', () => {drawer.hide(); tabGroup.show('models')}); 

    </script>
</body>
</html>