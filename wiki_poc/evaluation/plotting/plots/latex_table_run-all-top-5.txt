\begin{tabular}{llrr}
\toprule
model & model_class & size & accuracy \\
\midrule
wiki_predictions_gpt_4-175b & wiki_predictions_gpt_4 & 175.00 & 0.71 \\
gpt_3_5-175b & gpt_3_5 & 175.00 & 0.53 \\
flan_t5-11b & flan_t5 & 11.00 & 0.38 \\
incite_instruct-3b & incite_instruct & 3.00 & 0.37 \\
flan_t5-3b & flan_t5 & 3.00 & 0.36 \\
bloomz-7b1 & bloomz & 7.10 & 0.35 \\
flan_t5-0b780 & flan_t5 & 0.78 & 0.34 \\
bloomz-1b1 & bloomz & 1.10 & 0.32 \\
bloomz-1b7 & bloomz & 1.70 & 0.32 \\
flan_t5-0b250 & flan_t5 & 0.25 & 0.30 \\
bloomz-3b & bloomz & 3.00 & 0.30 \\
flan_t5-0b080 & flan_t5 & 0.08 & 0.28 \\
t5-3b & t5 & 3.00 & 0.27 \\
llama-7b & llama & 7.00 & 0.26 \\
t5-0b770 & t5 & 0.77 & 0.24 \\
roberta-0b355 & roberta & 0.35 & 0.21 \\
mpt-7b & mpt & 7.00 & 0.20 \\
mpt_instruct-6b7 & mpt_instruct & 6.70 & 0.20 \\
roberta-0b125 & roberta & 0.12 & 0.19 \\
distilbert_squad-0b062 & distilbert_squad & 0.06 & 0.17 \\
incite-3b & incite & 3.00 & 0.17 \\
t5-0b060 & t5 & 0.06 & 0.13 \\
roberta_squad-0b125 & roberta_squad & 0.12 & 0.08 \\
cerebras-13b & cerebras & 13.00 & 0.06 \\
falcon_instruct-7b & falcon_instruct & 7.00 & 0.05 \\
t5-0b220 & t5 & 0.22 & 0.04 \\
pythia-12b & pythia & 12.00 & 0.04 \\
cerebras-6b7 & cerebras & 6.70 & 0.04 \\
pythia-0b410 & pythia & 0.41 & 0.03 \\
gptj-6b & gptj & 6.00 & 0.03 \\
cerebras-1b3 & cerebras & 1.30 & 0.03 \\
pythia-2b8 & pythia & 2.80 & 0.03 \\
gpt_neox-20b & gpt_neox & 20.00 & 0.03 \\
falcon-7b & falcon & 7.00 & 0.03 \\
pythia-1b4 & pythia & 1.40 & 0.03 \\
cerebras-2b7 & cerebras & 2.70 & 0.03 \\
pythia-0b070 & pythia & 0.07 & 0.03 \\
roberta_squad-0b355 & roberta_squad & 0.35 & 0.02 \\
pythia-0b160 & pythia & 0.16 & 0.02 \\
pythia-6b9 & pythia & 6.90 & 0.02 \\
cerebras-0b111 & cerebras & 0.11 & 0.02 \\
distilbert-0b066 & distilbert & 0.07 & 0.01 \\
\bottomrule
\end{tabular}
