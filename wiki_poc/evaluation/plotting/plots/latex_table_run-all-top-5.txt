\begin{tabular}{lrr}
\toprule
model & size & accuracy \\
\midrule
flan_t5-11b & 11.000000 & 0.375474 \\
incite_instruct-3b & 3.000000 & 0.373789 \\
flan_t5-3b & 3.000000 & 0.363053 \\
bloomz-7b1 & 7.100000 & 0.350947 \\
flan_t5-0b780 & 0.780000 & 0.335789 \\
bloomz-1b1 & 1.100000 & 0.321158 \\
bloomz-1b7 & 1.700000 & 0.320105 \\
flan_t5-0b250 & 0.250000 & 0.303684 \\
bloomz-3b & 3.000000 & 0.302842 \\
flan_t5-0b080 & 0.080000 & 0.281684 \\
t5-3b & 3.000000 & 0.266000 \\
llama-7b & 7.000000 & 0.261579 \\
t5-0b770 & 0.770000 & 0.238421 \\
roberta-0b355 & 0.355000 & 0.206421 \\
mpt_instruct-6b7 & 6.700000 & 0.199895 \\
roberta-0b125 & 0.125000 & 0.190737 \\
distilbert_squad-0b062 & 0.062000 & 0.168842 \\
t5-0b060 & 0.060000 & 0.129684 \\
roberta_squad-0b125 & 0.125000 & 0.078421 \\
cerebras-13b & 13.000000 & 0.055789 \\
falcon_instruct-7b & 7.000000 & 0.047053 \\
t5-0b220 & 0.220000 & 0.043789 \\
pythia-12b & 12.000000 & 0.041158 \\
cerebras-6b7 & 6.700000 & 0.035684 \\
pythia-0b410 & 0.410000 & 0.034526 \\
gptj-6b & 6.000000 & 0.034105 \\
cerebras-1b3 & 1.300000 & 0.030947 \\
pythia-2b8 & 2.800000 & 0.030316 \\
gpt_neox-20b & 20.000000 & 0.030211 \\
falcon-7b & 7.000000 & 0.030000 \\
pythia-1b4 & 1.400000 & 0.029474 \\
cerebras-2b7 & 2.700000 & 0.026842 \\
pythia-0b070 & 0.070000 & 0.026211 \\
roberta_squad-0b355 & 0.355000 & 0.024947 \\
pythia-0b160 & 0.160000 & 0.023789 \\
pythia-6b9 & 6.900000 & 0.019895 \\
cerebras-0b111 & 0.111000 & 0.016842 \\
distilbert-0b066 & 0.066000 & 0.014000 \\
\bottomrule
\end{tabular}
