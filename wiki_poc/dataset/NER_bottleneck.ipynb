{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbec4d91",
   "metadata": {},
   "source": [
    "# Is Named Entity Recognition a bottleneck in masking entities?\n",
    "# Does paraphrasing move out too many wiki pages?\n",
    "\n",
    "To mask wikipedia texts, a NER model is used (dslim/bert-base-NER). Of 1000 random wikipedia articles about personalities, about half gets lost before further processing.\n",
    "-> lost when acquiring via wikiquery -> title -> find in dataset (some are not found..)\n",
    "-> lose more when paraphrasing deletes names\n",
    "-> lose more when NER does not detect !this is the bottleneck!\n",
    "**TODO** insert graphic from (https://sankeymatic.com/build/)\n",
    "\n",
    "Wiki-Text is reduced to 4096 characters for faster processing. 4k characters should be plenty to get several mentions of the personality of the article.\n",
    "\n",
    "\n",
    "NER vs Manual vs \"String Matching\"\n",
    "\n",
    "NER:             use a model\n",
    "string matching: use a very simple matching approach\n",
    "manual:          search and replace by hand\n",
    "\n",
    "\n",
    "### Open Questions\n",
    "- [ ] are 4096 characters enough to keep several mentions of the name of the person?\n",
    "- [ ] how many names are removed by paraphrasing?\n",
    "- [ ] how many names are removed in NER?\n",
    "- [ ] paraphrase text as a whole or as single sentences? -> single sentences keep more info, but might remove more names?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6240e727",
   "metadata": {},
   "source": [
    "### Step 1: How often does the title of the wiki page occur in the document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06099590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40a20bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('wiki-dataset-results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4205f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcec5d18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc1db29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3c3cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a11bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in 15 examples of paraphrased wiki texts\n",
    "df = pd.read_csv('15-wiki-samples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567ab218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = df.sample(15)\n",
    "# samples.to_csv('15-wiki-samples.csv')\n",
    "# samples\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdad26f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe6a4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = []\n",
    "for index, page in df.iterrows():\n",
    "    title = page['title']\n",
    "    text = page['raw']\n",
    "    counts.append(len(re.findall(title, text)))\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f20a2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.iloc[0]['raw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54332089",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = samples.iloc[0]['raw']\n",
    "title = samples.iloc[0]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffee886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d2a8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(title, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41947a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eb59fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea633764",
   "metadata": {},
   "source": [
    "### Step 2: check if mfwparserfrom hell + wikibot is better to work with\n",
    "no, use the sparqlwrapper to get persons from wikipedia, keep using datasets from huggingface to access cleaned wiki texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0354c670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7069a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db697786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the page titles of the queries persons\n",
    "names = [page['page_titleEN']['value'] for page in results['results']['bindings']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f84d830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\") # use train split, as it only has train, no other splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1d7943",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1259d8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import extract_text\n",
    "articles = extract_text(dataset, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c678bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(articles)\n",
    "articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc93bb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply NER to the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08793c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom.wiki import query_wiki_persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aed33fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_wiki_persons(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0028ce18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e46c0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380b0e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64160d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc9528ee",
   "metadata": {},
   "source": [
    "## time executions of fill-mask (array of inputs vs single inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4863aa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9430d847",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\"Hello, I am a text with a <mask>\", \"Wow, there is <mask> more text.\",\n",
    "          \"The largest building in <mask> is the taj mahal.\", \"Let's see if <mask> can win the f1 championship.\",\n",
    "         \"<mask> are the only animals to live in these rough conditions.\", \"Not many people can say the have seen <mask>.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bccefc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask = pipeline(\"fill-mask\", model=\"roberta-base\", tokenizer='roberta-base', top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bf0b735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334 ms ± 12.2 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r 30 [fill_mask(x) for x in inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2feb05d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352 ms ± 22.6 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r 30 fill_mask(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2625997b",
   "metadata": {},
   "source": [
    "### time executions of paraphrase (array of inputs vs single inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4730b0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12eafb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_model(model_name='tuner007/pegasus_paraphrase'):\n",
    "    print(f\"Loading {model_name}\")\n",
    "    tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "    model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fc7b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de0aeb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff087a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
